{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "from predicate_construction import construct_predicates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in GoodReads dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = '/scratch/charles/RankPSL/data_construction/goodreads_data_construction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name, head = 1000):\n",
    "    count = 0\n",
    "    data = []\n",
    "    with gzip.open(file_name) as fin:\n",
    "        for l in fin:\n",
    "            d = json.loads(l)\n",
    "            count += 1\n",
    "            data.append(d)\n",
    "            \n",
    "            # break if reaches the threshold number of lines\n",
    "            if (head) and (count > head):\n",
    "                break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = load_data(os.path.join(DIR, 'goodreads_books_comics_graphic.json.gz'), head=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = load_data(os.path.join(DIR, 'goodreads_interactions_comics_graphic.json.gz'), head=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = load_data(os.path.join(DIR, 'goodreads_reviews_comics_graphic.json.gz'), head=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pandas frame for each dataset\n",
    "books_df = pd.DataFrame(books)\n",
    "books_df = books_df.set_index('book_id')\n",
    "books_df.average_rating = books_df.average_rating.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = pd.DataFrame(interactions)\n",
    "interactions_df = interactions_df.set_index(['user_id', 'book_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# development sample from frame\n",
    "interactions_df = interactions_df.sample(n=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.DataFrame(reviews)\n",
    "reviews_df = reviews_df.set_index(['review_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply preprocessing mentioned in paper: \n",
    "\n",
    "\"We apply the same preprocessing criteria for all five datasets: we\n",
    "discard users who have never reached the last stage of any behavior\n",
    "chain and items with fewer than 5 associated interactions in\n",
    "the system. Statistics and distributions of the above datasets after\n",
    "preprocessing are included in Table 2. For each dataset, we sample\n",
    "100,000 interaction chains for validation and another 100,000\n",
    "for testing. Within each of these two sets, each interaction chain\n",
    "corresponds to a different user. Data and code are available at\n",
    "https://github.com/MengtingWan/chainRec.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = interactions_df.loc[reviews_df.user_id.unique()]\n",
    "interactions_df = interactions_df.groupby('book_id').filter(lambda book_frame: book_frame.shape[0] > 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.index.get_level_values(0).unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out books that are not in interactions dataframe\n",
    "# is this leaking information since I am filtering the books before train test split?\n",
    "books_df = books_df[books_df.index.isin(interactions_df.index.unique(level='book_id'))]\n",
    "books_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out reviews by users not in interactions dataframe\n",
    "# pull out reviews for book not in interactions dataframe\n",
    "reviews_df = reviews_df.loc[reviews_df.book_id.isin(interactions_df.index.unique(level='book_id'))]\n",
    "reviews_df = reviews_df.loc[reviews_df.user_id.isin(interactions_df.index.unique(level='user_id'))]\n",
    "reviews_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_memoize_pandas(s):\n",
    "    \"\"\"\n",
    "    Memoization technique\n",
    "    \"\"\"\n",
    "    dates = {date:pd.to_datetime(date, infer_datetime_format=True, utc=True) for date in s.unique()}\n",
    "    return s.map(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactions\n",
    "interactions_df.date_added = f_memoize_pandas(interactions_df.date_added)\n",
    "interactions_df.date_updated = f_memoize_pandas(interactions_df.date_updated)\n",
    "interactions_df.read_at = f_memoize_pandas(interactions_df.read_at)\n",
    "interactions_df.started_at = f_memoize_pandas(interactions_df.started_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews\n",
    "reviews_df.date_added = f_memoize_pandas(reviews_df.date_added)\n",
    "reviews_df.date_updated = f_memoize_pandas(reviews_df.date_updated)\n",
    "reviews_df.read_at = f_memoize_pandas(reviews_df.read_at)\n",
    "reviews_df.started_at = f_memoize_pandas(reviews_df.started_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# books\n",
    "books_df.ratings_count = pd.to_numeric(books_df.ratings_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(books_df.shape)\n",
    "books_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(interactions_df.shape)\n",
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(reviews_df.shape)\n",
    "reviews_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of dates of date_added interactions\n",
    "display(interactions_df[\"date_added\"].min())\n",
    "display(interactions_df[\"date_added\"].max())\n",
    "interactions_df.groupby([interactions_df[\"date_added\"].dt.year])[\"rating\"].count().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of dates of date_updated interactions\n",
    "display(interactions_df[\"date_updated\"].min())\n",
    "display(interactions_df[\"date_updated\"].max())\n",
    "interactions_df.groupby([interactions_df[\"date_updated\"].dt.year])[\"rating\"].count().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let every interaction with a book **starting** after 2017 be the test set.\n",
    "\n",
    "There looks like there is enough interactions to suffice for a test set.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition train and test interaction data by 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition train and test data by 2017\n",
    "test_interactions_df = interactions_df[interactions_df.date_added.dt.year >= 2017]\n",
    "test_reviews_df = reviews_df[reviews_df.date_added.dt.year >= 2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_interactions_df[test_interactions_df.rating != 0].groupby(\"rating\").rating.count().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the 0 ratings, those represent that the user has not yet provided their rating \n",
    "test_interactions_df = test_interactions_df[test_interactions_df.rating != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train set\n",
    "train_interactions_df = interactions_df[interactions_df.date_added.dt.year < 2017]\n",
    "train_reviews_df = reviews_df[reviews_df.date_added.dt.year < 2017]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method for running PSL CLI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_reads_psl(books_df, interactions_df, reviews_df, \n",
    "                   obs_interactions, obs_reviews,\n",
    "                   target_interactions, target_reviews,\n",
    "                   truth_interactions, truth_reviews, \n",
    "                   fold, setting):\n",
    "    \"\"\"\n",
    "    Given the provided datasets, run PSL \n",
    "    \"\"\"\n",
    "    \n",
    "    #call predicate construction method and provide sampled dataset \n",
    "    construct_predicates(books_df, interactions_df, reviews_df,\n",
    "                         obs_interactions, obs_reviews,\n",
    "                         target_interactions, target_reviews,\n",
    "                         truth_interactions, truth_reviews,\n",
    "                         fold, setting)\n",
    "    \n",
    "    # move the data to the cli data dir\n",
    "    ! rm -r ../../datasets/goodreads/data/goodreads\n",
    "    ! mv ./goodreads ../../datasets/goodreads/data/goodreads\n",
    "    \n",
    "    # perform PSL inference\n",
    "    !(cd ../../ ; ./run.sh )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full dataset predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 10)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference experiments on datasets: [goodreads].\n",
      "\n",
      "Running goodreads (#0) -- .\n",
      "mv: cannot stat './inferred-predicates': No such file or directory\n",
      "Running goodreads (#0) -- .\n",
      "mv: cannot stat './inferred-predicates': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# set fold and setting\n",
    "fold = '0'\n",
    "setting = 'eval'\n",
    "\n",
    "# run PSL eval\n",
    "good_reads_psl(books_df, interactions_df, reviews_df, \n",
    "               train_interactions_df.index, train_reviews_df.index,\n",
    "               test_interactions_df.index, test_reviews_df.index,\n",
    "               test_interactions_df.index, test_reviews_df.index, \n",
    "               fold, setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.groupby(books_df.language_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomly select observed and unobserved interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_data_sample(data_frame, n):\n",
    "    return data_frame.sample(n).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chronological_data_sample(data_frame, n):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I would like a principled way to sample this dataset so we can \n",
    "\n",
    "1. get a fair estimate of generalization performance\n",
    "2. perform cross validation\n",
    "3. perform inferences on many small graphs that are good representatives of larger distribution to learn weights\n",
    "4. perform inferences on many small graphs that are good representatives of larger distribution to output an ensemble of inference predictions that can be aggregated by a voting procedure\n",
    "5. Combine a mixture of psl models. Each PSL model defines a distribution. The mixtures could come from a random walk sampling seeded at each of the targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "This is difficult, and I am making a lot of assumptions in order to sample from this multilayer network. There is interesting work to be done here but perhaps a more productive first pass is using the entire dataset and then varying what is observed and unobserved for each weight learning inference iteration. Then we can see if weight learning variance is controlled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models for predicting latent preferences are influenced by the structure of the graphs induced by the\n",
    "\n",
    "    - Relations amongst the books\n",
    "        - Authors relationship\n",
    "        - Genre relationship\n",
    "        - Series relationship\n",
    "        - user feedback patterns\n",
    "        \n",
    "    - Similarities amongst the users\n",
    "        - feedback similarites\n",
    "            - shelving \n",
    "            - read \n",
    "            - rating \n",
    "            - review \n",
    "        \n",
    "Perhaps we can sample both graphs induced by these relationship by seperately seeding a random walk at the targets and letting the random walk spread until a prespecified graph size is met.\n",
    "\n",
    "Then we can join these sampled graphs so that only users from the user sample and books from the book graph sample are used.\n",
    "\n",
    "To ensure information is not leaked across folds, we should note that the user similarities are based on their ratings of the books.\n",
    "\n",
    "*Note* that structural information is really only available among the books otherwise, the user relationships are latent and can be infered via rating habits.\n",
    "\n",
    "Perhaps first pass we sample from the book data set then randomly sample from users and filter so only books that were found in graph sampling are used in PSL model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create author to book data frame\n",
    "author_df = pd.DataFrame(columns=books_df.index)\n",
    "author_df.index.name = 'author_id'\n",
    "for book_id, book in books_df.iterrows():\n",
    "    for author in book.authors:\n",
    "        author_df.loc[author['author_id'], book.name] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from author information create book_book_by_author_adj_matrix\n",
    "author_df = author_df.fillna(0)\n",
    "book_author_matrix = author_df.to_numpy().transpose()\n",
    "np.matmul(book_author_matrix, book_author_matrix.transpose()).shape\n",
    "book_book_by_author_adj_matrix = pd.DataFrame(np.matmul(book_author_matrix, book_author_matrix.transpose()), \n",
    "                                              index=books_df.index, columns=books_df.index)\n",
    "book_book_by_author_adj_matrix[book_book_by_author_adj_matrix >= 1] = 1\n",
    "\n",
    "# remove self loops\n",
    "book_book_by_author_adj_matrix = (book_book_by_author_adj_matrix - np.eye(book_book_by_author_adj_matrix.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create genre to book data frame, genre is from shelf names by users\n",
    "max_genres = 3\n",
    "genre_df = pd.DataFrame(columns=books_df.index)\n",
    "genre_df.index.name = 'genre'\n",
    "for book_id, book in books_df.iterrows():\n",
    "    n_genres = 0\n",
    "    for genre in book.popular_shelves:\n",
    "        if n_genres >= 3:\n",
    "            break\n",
    "        else:\n",
    "            if genre['name'] != 'to-read':\n",
    "                genre_df.loc[genre['name'], book.name] = 1\n",
    "                n_genres = n_genres + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from genre information create book_book_by_genre_adj_matrix\n",
    "genre_df = genre_df.fillna(0)\n",
    "book_genre_matrix = genre_df.to_numpy().transpose()\n",
    "np.matmul(book_genre_matrix, book_genre_matrix.transpose()).shape\n",
    "book_book_by_genre_adj_matrix = pd.DataFrame(np.matmul(book_genre_matrix, book_genre_matrix.transpose()),\n",
    "                                             index=books_df.index, columns=books_df.index)\n",
    "book_book_by_genre_adj_matrix[book_book_by_genre_adj_matrix >= 1] = 1\n",
    "\n",
    "# remove self loops\n",
    "book_book_by_genre_adj_matrix = (book_book_by_genre_adj_matrix - np.eye(book_book_by_genre_adj_matrix.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create series to book data frame\n",
    "series_df = pd.DataFrame(columns=books_df.index)\n",
    "series_df.index.name = 'series'\n",
    "for book_id, book in books_df.iterrows():\n",
    "    for series in book.series:\n",
    "        series_df.loc[series, book.name] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from genre information create book_book_by_genre_adj_matrix\n",
    "series_df = series_df.fillna(0)\n",
    "book_series_matrix = series_df.to_numpy().transpose()\n",
    "np.matmul(book_series_matrix, book_series_matrix.transpose()).shape\n",
    "book_book_by_series_adj_matrix = pd.DataFrame(np.matmul(book_series_matrix, book_series_matrix.transpose()),\n",
    "                                             index=books_df.index, columns=books_df.index)\n",
    "book_book_by_series_adj_matrix[book_book_by_series_adj_matrix >= 1] = 1\n",
    "\n",
    "# remove self loops\n",
    "book_book_by_series_adj_matrix = (book_book_by_series_adj_matrix - np.eye(book_book_by_series_adj_matrix.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Graph Transition Matrix**\n",
    "\n",
    "A first pass approach is by simply overlapping the adjacency matrices, i.e. summing them, and then normalizing each row (or column) so the row values add to 1. We can then interpret these values as transition probabilities to perform a random walk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_graph_transition = (book_book_by_author_adj_matrix + book_book_by_genre_adj_matrix + book_book_by_series_adj_matrix)\n",
    "book_graph_transition[book_graph_transition < 0] = 0\n",
    "book_graph_transition = book_graph_transition / book_graph_transition.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop interactions for which there is no transition probabilities between the books in the test and train set\n",
    "non_degenerate_books = book_graph_transition[~(book_graph_transition.isna().sum(axis=0) > 0)].index\n",
    "book_graph_transition = book_graph_transition.loc[non_degenerate_books, non_degenerate_books]\n",
    "test_interactions_df = test_interactions_df.loc[(slice(None), book_graph_transition.index), :]\n",
    "test_reviews_df = test_reviews_df[test_reviews_df.book_id.isin(book_graph_transition.index)]\n",
    "\n",
    "train_interactions_df = train_interactions_df.loc[(slice(None), book_graph_transition.index), :]\n",
    "train_reviews_df = train_reviews_df[train_reviews_df.book_id.isin(book_graph_transition.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write train, test, and all graph to a csv file to visualize in gephi for debugging purposes\n",
    "train_books = train_interactions_df.index.get_level_values(level=1)\n",
    "test_books = train_interactions_df.index.get_level_values(level=1)\n",
    "\n",
    "train_book_graph = book_graph_transition.loc[train_books, train_books]\n",
    "test_book_graph = book_graph_transition.loc[test_books, test_books]\n",
    "\n",
    "train_book_graph.to_csv('./train_book_graph.csv')\n",
    "test_book_graph.to_csv('./test_book_graph.csv')\n",
    "book_graph_transition.to_csv('book_graph.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method for sampling from dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goodreads_train_sample(n, initial_state=None):\n",
    "    \"\"\"\n",
    "    Method for sampling from the goodreads dataset by performing a random walk on the book graph\n",
    "    starting at the initial state(s) provided\n",
    "    n: number of steps on the graph for each initial state\n",
    "    initial_state: array of starting points for random walk on graph\n",
    "    return: Sampled books_df, interactions_df, and reviews_df \n",
    "    \"\"\"\n",
    "    # set initial state, if it was not passed, to a single random book\n",
    "    if initial_state is None:\n",
    "        initial_state = np.random.choice(books_df.index, 1)\n",
    "        \n",
    "    # obtain random walk sample of books\n",
    "    sampled_books = set(list(initial_state))\n",
    "    for state_0 in initial_state:\n",
    "        state = state_0\n",
    "        for i in range(n):\n",
    "            transition_dist = book_graph_transition[state]\n",
    "            state = np.random.choice(book_graph_transition.index, 1, p=transition_dist)[0]\n",
    "            sampled_books.add(state)\n",
    "            \n",
    "    # Write sampled graph to a csv file to visualize in gephi for debugging purposes\n",
    "    sampled_book_graph = book_graph_transition.loc[sampled_books, sampled_books]\n",
    "    sampled_book_graph.to_csv('./sampled_book_graph.csv')\n",
    "\n",
    "    # use books to sample datasets\n",
    "    sampled_books_df = books_df.loc[sampled_books, :]\n",
    "    sampled_interactions_df = train_interactions_df.loc[(slice(None), sampled_books), :]\n",
    "    sampled_reviews_df = train_reviews_df[train_reviews_df.book_id.isin(sampled_books)]\n",
    "\n",
    "    return sampled_books_df, sampled_interactions_df, sampled_reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does including more relationship information influence inference performance?\n",
    "\n",
    "Relationship information enters the model through\n",
    "\n",
    "- Structured input\n",
    "    - Scale the amount of graph sampling\n",
    "    - Scale the amount of blocking\n",
    "- Structured output\n",
    "    - Scale the amount of blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample From Dataset and run PSL in eval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from dataset using the books in the test set\n",
    "test_books = test_interactions_df.index.unique(level='book_id')\n",
    "sampled_books_df, sampled_interactions_df, sampled_reviews_df = goodreads_train_sample(5, test_books)\n",
    "\n",
    "# partition by obs_interactions, target_interactions, and, truth_interactions\n",
    "obs_interactions = sampled_interactions_df.index\n",
    "truth_interactions = test_interactions_df.index\n",
    "target_interactions = truth_interactions\n",
    "\n",
    "# partition by obs_reviews, target_reviews, and, truth_reviews\n",
    "obs_reviews = sampled_reviews_df.index\n",
    "truth_reviews = test_reviews_df.index\n",
    "target_reviews = truth_reviews\n",
    "\n",
    "# set fold and setting\n",
    "fold = '0'\n",
    "setting = 'eval'\n",
    "\n",
    "# run PSL eval\n",
    "good_reads_psl(books_df, interactions_df, reviews_df, \n",
    "               obs_interactions, obs_reviews,\n",
    "               target_interactions, target_reviews,\n",
    "               truth_interactions, truth_reviews, \n",
    "               fold, setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_interactions_df = interactions_df.loc[obs_interactions, :]\n",
    "rating_series = observed_interactions_df[observed_interactions_df.rating > 0]['rating']\n",
    "rating_series = rating_series - rating_series.min()\n",
    "rating_series = rating_series / rating_series.max()\n",
    "rating_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How variable is weight learning across dataset samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does weight learning's effectiveness change with dataset size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does weight learning with many different data samples effect weight learning result and effectiveness?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRATCH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>is_read</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text_incomplete</th>\n",
       "      <th>date_added</th>\n",
       "      <th>date_updated</th>\n",
       "      <th>read_at</th>\n",
       "      <th>started_at</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ad334e724ac43beec8b46ffa49ccc928</th>\n",
       "      <th>6407014</th>\n",
       "      <td>a76a4c4cb9d218bd3ccc49b7219c205f</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2016-03-27 05:15:19+00:00</td>\n",
       "      <td>2016-03-27 05:15:20+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1ec1f3813fe7b2adfb05c5554f914cc6</th>\n",
       "      <th>15195</th>\n",
       "      <td>cc97eb5e1b2a4ac09abcb193de0e6ee2</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>Maus is one of those graphic novels that even ...</td>\n",
       "      <td>2017-10-07 06:11:05+00:00</td>\n",
       "      <td>2017-10-16 00:56:36+00:00</td>\n",
       "      <td>2017-10-08 14:41:49+00:00</td>\n",
       "      <td>2017-10-07 06:11:06+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d5d065cf2549fbfce44a5984b97ffe15</th>\n",
       "      <th>31217834</th>\n",
       "      <td>dd4f3b35a761b19e0a647c52178881e3</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>So, I'm the asshole that really enjoyed this a...</td>\n",
       "      <td>2017-01-17 04:58:07+00:00</td>\n",
       "      <td>2017-01-17 05:33:47+00:00</td>\n",
       "      <td>2017-01-16 08:00:00+00:00</td>\n",
       "      <td>2017-01-16 08:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c79d0fd40f7cb69b8f724d1567e14ba2</th>\n",
       "      <th>8477057</th>\n",
       "      <td>6bf7a288e41e4a8106be1fcb8498611d</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2012-08-07 15:57:38+00:00</td>\n",
       "      <td>2012-08-07 15:57:38+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1e5028f077c905873f90706a600499fc</th>\n",
       "      <th>18339834</th>\n",
       "      <td>c12f2d7a35c9234a106fd20ca1543063</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>2014-12-13 15:07:18+00:00</td>\n",
       "      <td>2014-12-13 15:07:29+00:00</td>\n",
       "      <td>2014-12-13 15:07:29+00:00</td>\n",
       "      <td>2014-12-13 15:07:18+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  review_id  \\\n",
       "user_id                          book_id                                      \n",
       "ad334e724ac43beec8b46ffa49ccc928 6407014   a76a4c4cb9d218bd3ccc49b7219c205f   \n",
       "1ec1f3813fe7b2adfb05c5554f914cc6 15195     cc97eb5e1b2a4ac09abcb193de0e6ee2   \n",
       "d5d065cf2549fbfce44a5984b97ffe15 31217834  dd4f3b35a761b19e0a647c52178881e3   \n",
       "c79d0fd40f7cb69b8f724d1567e14ba2 8477057   6bf7a288e41e4a8106be1fcb8498611d   \n",
       "1e5028f077c905873f90706a600499fc 18339834  c12f2d7a35c9234a106fd20ca1543063   \n",
       "\n",
       "                                           is_read  rating  \\\n",
       "user_id                          book_id                     \n",
       "ad334e724ac43beec8b46ffa49ccc928 6407014     False       0   \n",
       "1ec1f3813fe7b2adfb05c5554f914cc6 15195        True       5   \n",
       "d5d065cf2549fbfce44a5984b97ffe15 31217834     True       4   \n",
       "c79d0fd40f7cb69b8f724d1567e14ba2 8477057     False       0   \n",
       "1e5028f077c905873f90706a600499fc 18339834     True       4   \n",
       "\n",
       "                                                                      review_text_incomplete  \\\n",
       "user_id                          book_id                                                       \n",
       "ad334e724ac43beec8b46ffa49ccc928 6407014                                                       \n",
       "1ec1f3813fe7b2adfb05c5554f914cc6 15195     Maus is one of those graphic novels that even ...   \n",
       "d5d065cf2549fbfce44a5984b97ffe15 31217834  So, I'm the asshole that really enjoyed this a...   \n",
       "c79d0fd40f7cb69b8f724d1567e14ba2 8477057                                                       \n",
       "1e5028f077c905873f90706a600499fc 18339834                                                      \n",
       "\n",
       "                                                         date_added  \\\n",
       "user_id                          book_id                              \n",
       "ad334e724ac43beec8b46ffa49ccc928 6407014  2016-03-27 05:15:19+00:00   \n",
       "1ec1f3813fe7b2adfb05c5554f914cc6 15195    2017-10-07 06:11:05+00:00   \n",
       "d5d065cf2549fbfce44a5984b97ffe15 31217834 2017-01-17 04:58:07+00:00   \n",
       "c79d0fd40f7cb69b8f724d1567e14ba2 8477057  2012-08-07 15:57:38+00:00   \n",
       "1e5028f077c905873f90706a600499fc 18339834 2014-12-13 15:07:18+00:00   \n",
       "\n",
       "                                                       date_updated  \\\n",
       "user_id                          book_id                              \n",
       "ad334e724ac43beec8b46ffa49ccc928 6407014  2016-03-27 05:15:20+00:00   \n",
       "1ec1f3813fe7b2adfb05c5554f914cc6 15195    2017-10-16 00:56:36+00:00   \n",
       "d5d065cf2549fbfce44a5984b97ffe15 31217834 2017-01-17 05:33:47+00:00   \n",
       "c79d0fd40f7cb69b8f724d1567e14ba2 8477057  2012-08-07 15:57:38+00:00   \n",
       "1e5028f077c905873f90706a600499fc 18339834 2014-12-13 15:07:29+00:00   \n",
       "\n",
       "                                                            read_at  \\\n",
       "user_id                          book_id                              \n",
       "ad334e724ac43beec8b46ffa49ccc928 6407014                        NaT   \n",
       "1ec1f3813fe7b2adfb05c5554f914cc6 15195    2017-10-08 14:41:49+00:00   \n",
       "d5d065cf2549fbfce44a5984b97ffe15 31217834 2017-01-16 08:00:00+00:00   \n",
       "c79d0fd40f7cb69b8f724d1567e14ba2 8477057                        NaT   \n",
       "1e5028f077c905873f90706a600499fc 18339834 2014-12-13 15:07:29+00:00   \n",
       "\n",
       "                                                         started_at  \n",
       "user_id                          book_id                             \n",
       "ad334e724ac43beec8b46ffa49ccc928 6407014                        NaT  \n",
       "1ec1f3813fe7b2adfb05c5554f914cc6 15195    2017-10-07 06:11:06+00:00  \n",
       "d5d065cf2549fbfce44a5984b97ffe15 31217834 2017-01-16 08:00:00+00:00  \n",
       "c79d0fd40f7cb69b8f724d1567e14ba2 8477057                        NaT  \n",
       "1e5028f077c905873f90706a600499fc 18339834 2014-12-13 15:07:18+00:00  "
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_interactions = train_interactions_df.index\n",
    "target_interactions = train_interactions_df.index\n",
    "truth_interactions = test_interactions_df.index\n",
    "\n",
    "# observed predicates\n",
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_top_k_sim = item_item_sim.apply(pd.Series.nlargest, n=5).stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_item_series = pd.Series(data=1, index=book_top_k_sim.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          book_id \n",
       "10088114  10088114    1\n",
       "          5989573     1\n",
       "          17412749    1\n",
       "1011359   1011359     1\n",
       "10138607  10138607    1\n",
       "                     ..\n",
       "9876989   13536803    1\n",
       "          9876989     1\n",
       "991197    991197      1\n",
       "9921208   138396      1\n",
       "          9921208     1\n",
       "Length: 2415, dtype: int64"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_item_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
